
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>How to use this repo &#8212; Deep learning on SAR data 1 documentation</title>
    <link rel="stylesheet" href="_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Project description" href="Project-description.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="how-to-use-this-repo">
<h1>How to use this repo<a class="headerlink" href="#how-to-use-this-repo" title="Permalink to this headline">¶</a></h1>
<div class="section" id="project-structure">
<h2>Project structure<a class="headerlink" href="#project-structure" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li>docs</li>
<li>data</li>
<li>src</li>
<li>manuscript</li>
<li>build</li>
</ul>
<p>The description of the structure and processing steps follows.</p>
<div class="section" id="datasets">
<h3>Datasets<a class="headerlink" href="#datasets" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li>data<ul>
<li>SAR_preprocess</li>
<li>DL_models</li>
<li>DL_output</li>
<li>DL_prepare</li>
</ul>
</li>
</ul>
<p>The <em>data</em> folder holds all the data used in the project. Below, we discuss each of the subfolders under it.</p>
<div class="section" id="sar-preprocessing">
<h4>SAR preprocessing<a class="headerlink" href="#sar-preprocessing" title="Permalink to this headline">¶</a></h4>
<ul>
<li><p class="first">SAR_preprocess</p>
<blockquote>
<div><ul class="simple">
<li>orig-data</li>
<li>RES</li>
<li>finn_dem_WGS84.tif</li>
</ul>
</div></blockquote>
</li>
</ul>
<p>This folder holds the data needed for the first step in the pipeline, i.e., for SAR data preprocessing. In this step, we take SAR images with two polarisation channels (single, such as HH or VV and cross-pol, such as HV and VH) and multilook, calibrate, terrain-flatten and terrain-correct them. Finally, we produce the resulting RGB composites of them so that each of the <em>Gamma0</em> channels is saved in the first two bands, and the corresponding digital elevation model (DEM) is saved in the last band.</p>
<p><em>SAR_preprocess</em> holds original SAR images in <em>orig-data</em> and the DEM model <em>finn_dem_WGS84.tif</em> used for the terrain correction.</p>
<p>SAR image preprocessing is performed using the ESA <em>snap tool</em>. The preprocessing graph <em>SAR_preprocessing_Graph_v1.xml</em>, which can be used by the tool, is found under <em>src/SAR-data-preprocess/code_snap</em>. You can perform the preprocessing by first opening the downloaded SAR images in SNAP (you can open even without decompressing them, i.e., in the original <em>.zip</em> format). Then select <em>Tools -&gt; Batch Processing</em> and press the second “+” sign from the top to populate the batch with all the images. After that, press <em>Load Graph</em> and open the graph file <em>SAR_preprocessing_Graph_v1.xml</em>. Make sure that the output directory points to <em>SAR_preprocess/RES</em> on your machine and that the type of output files selected under <em>Save as:</em> is <em>GeoTIFF-BigTIFF</em>. This is needed because the processing graph creates large RGB composites, the size of which is often more then 4GB. Then press <em>Run</em>. Depending on the number of input images and your machine, this might take several hours.</p>
<p>After they are produced, the resulting RGB composite tiff files can be MOVED TO <em>DL_prepare/prepared-SAR-tiffs</em> for the additional preprocessing step required by the DL models.</p>
</div>
<div class="section" id="preprocesing-for-dl">
<h4>Preprocesing for DL<a class="headerlink" href="#preprocesing-for-dl" title="Permalink to this headline">¶</a></h4>
<ul>
<li><p class="first">DL_prepare</p>
<blockquote>
<div><ul class="simple">
<li>Corine</li>
<li>cropped-SAR-tiffs</li>
<li>labels</li>
<li>prepared-SAR-tiffs</li>
<li>train</li>
</ul>
</div></blockquote>
</li>
</ul>
<p>It is generally a good idea to prepare the images for the DL models in the following way. Most of the models expect square-shaped images, so we crop each tiff file into a number of square pieces that have the size around 1000x1000 pixels, but not smaller then 800 pixels in any of the dimensions (the number is chosen in our case also because of the limitation of the GPU computing resources, and it can be changed on other machines).</p>
<p>Converting SAR band values into <em>decibels</em> is a regular SAR preprocessing step, however, due to a more convenient implementation like this, we perform that step under preprocessing for DL.
In addition, each band should be <em>normalized</em> so that the distribution of the pixel values would resemble a Gaussian distribution centered at zero. This makes convergence faster while training the DL models. Data normalization is done by subtracting the mean from each pixel, and then dividing the result by the standard deviation. In addition, given that our DL models expect pixel values in the range (0,255), we then apply <em>scaling</em> the normalized data to that range. Such preprocessed data are then found in the folder <em>train</em>.</p>
<p>Also, we need to provide the labels for each training/test image, and this is done by cropping corrsponding pieces of the Corine mask (found in the folder <em>Corine</em>). The pieces should correspond to the geographic area of the respective train/test images. Resulting label images are stored in the folder <em>labels</em>. Note that training/test images and corresponding labels have <em>exactly the same names</em>, as that is what the DL models suite we use expects for their input.</p>
<p>Finally, the ready data are fed into <em>DL_models</em> directory using the script <em>divide_train_test.py</em>. This scripts will split the data into train, test, and validation sets.</p>
</div>
<div class="section" id="deep-learning-models">
<h4>Deep Learning models<a class="headerlink" href="#deep-learning-models" title="Permalink to this headline">¶</a></h4>
<ul>
<li><p class="first">DL_models</p>
<blockquote>
<div><ul class="simple">
<li>Semantic-Segmentation-Suite</li>
</ul>
</div></blockquote>
</li>
</ul>
<p>All the models that we use come from the <a class="reference external" href="https://github.com/GeorgeSeif/Semantic-Segmentation-Suite">Semantic Segmentation Suite GitHub repository</a>.</p>
<p>In order for one to use this semantic segmentation suite, the data should be placed in a folder within the root directory <em>Semantic-Segmentation-Suite</em> (we call this folder <em>Corine-Sentinel-DEM</em>) and structured as follows:</p>
<ul class="simple">
<li><dl class="first docutils">
<dt>Semantic-Segmentation-Suite/Corine-Sentinel-DEM</dt>
<dd><ul class="first last">
<li>test</li>
<li>test-labels</li>
<li>train</li>
<li>train-labels</li>
<li>val</li>
<li>val-labels</li>
<li>class_dict.csv</li>
</ul>
</dd>
</dl>
</li>
</ul>
<p>Invoking the script <em>src/prepare-data-for-DL/divide_train_test.py</em> populates the folder structure above. The file <em>class_dict.csv</em> provides the mapping between the class names and the label rgb colors.</p>
<p>With such a structure ready, we can invoke and test a number of the algorithms available under the suite using its <em>main.py</em> script with the appropriate arguments. The <em>Readme</em> file of the <em>Semantic-Segmentation-Suite</em> itself provides more information on this for each model.</p>
</div>
<div class="section" id="deep-learning-results">
<h4>Deep Learning Results<a class="headerlink" href="#deep-learning-results" title="Permalink to this headline">¶</a></h4>
<ul>
<li><p class="first">DL_output</p>
<blockquote>
<div><ul class="simple">
<li>orig</li>
<li>gt</li>
<li>pred</li>
</ul>
</div></blockquote>
</li>
</ul>
<p>Once you have developed and tested the models, you can receive results of the best/selected among them to the folder <em>DL_output</em>. This is done by invoking the script <em>src/postprocess-DL-res/georef_results.sh</em>, which in addition to copying select outputs also georefernces the results (prediction outputs) so that we can visualize them using GIS tools.</p>
</div>
</div>
<div class="section" id="source-code">
<h3>Source code<a class="headerlink" href="#source-code" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p class="first">src</p>
<blockquote>
<div><ul class="simple">
<li>postprocess-DL-res</li>
<li>prepare-data-for-DL</li>
<li>SAR-data-preprocess</li>
</ul>
</div></blockquote>
</li>
</ul>
<div class="section" id="sar-data-preprocessing">
<h4>SAR data preprocessing<a class="headerlink" href="#sar-data-preprocessing" title="Permalink to this headline">¶</a></h4>
<p>This folder holds only the .xml file for the processing graph to be used in SNAP, as described under SAR preprocessing (see <span class="xref std std-ref">SARprep</span>).</p>
</div>
<div class="section" id="preparing-data-for-dl">
<h4>Preparing data for DL<a class="headerlink" href="#preparing-data-for-dl" title="Permalink to this headline">¶</a></h4>
<ul>
<li><p class="first">prepare-data-for-DL</p>
<blockquote>
<div><ul class="simple">
<li>main.py</li>
<li>…</li>
<li>divide_train_test.py</li>
</ul>
</div></blockquote>
</li>
</ul>
<p>Invoking <em>main.py</em> will run the preprocessing steps described above (see Preprocesing for DL), and invoking <em>divide_train_test.py</em> will place those data in the right folder and proportions under <em>Semantic-Segmentation-Suite/Corine-Sentinel-DEM</em>.</p>
</div>
<div class="section" id="postprocessing-data-after-dl">
<h4>Postprocessing data after DL<a class="headerlink" href="#postprocessing-data-after-dl" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>postprocess-DL-res</li>
</ul>
<p>The script <em>georef_and_copy_results.py</em> will georefernce results (prediction .pngs) and place them under <em>data/DL_output</em>. It will also crop the test files to the same dimensions as the predicitons (1024x1024) and copy all to corresponding folders in <em>DL_output</em>.</p>
</div>
</div>
<div class="section" id="documentation">
<h3>Documentation<a class="headerlink" href="#documentation" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li>docs<ul>
<li>_build</li>
<li>_static</li>
<li>_templates</li>
<li>conf.py</li>
<li>.txt</li>
</ul>
</li>
</ul>
<p>The <em>docs</em> folder holds resources for this documentation you are reading. We use <em>sphinx</em> + <em>.rst</em> to create it. In <em>_build</em> you can find the root documentation file <em>index.html</em>. Starting from it, you get the links to the rest of the documentation.</p>
</div>
</div>
</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">Deep learning on SAR data</a></h1>








<h3>Navigation</h3>
<p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="Project-description.html">Project description</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">How to use this repo</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#project-structure">Project structure</a></li>
</ul>
</li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="Project-description.html" title="previous chapter">Project description</a></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2018, sanja7s (Sanja Scepanovic).
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.8.1</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.11</a>
      
      |
      <a href="_sources/How-to.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>